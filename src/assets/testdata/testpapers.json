[
    {
        "abstract": "This paper studies offline Imitation Learning (IL) where an agent learns to imitate an expert demonstrator without additional online environment interactions. In- stead, the learner is presented with a static offline dataset of state-action-next state transition triples from a potentially less proficient behavior policy. We introduce Model-based IL from Offline data (MILO): an algorithmic framework that utilizes the static dataset to solve the offline IL problem efficiently both in theory and in practice. In theory, even if the behavior policy is highly sub-optimal compared to the expert, we show that as long as the data from the behavior policy provides sufficient coverage on the expert state-action traces (and with no necessity for a global coverage over the entire state-action space), MILO can provably combat the covariate shift issue in IL. Complementing our theory results, we also demon- strate that a practical implementation of our approach mitigates covariate shift on benchmark MuJoCo continuous control tasks. We demonstrate that with behavior policies whose performances are less than half of that of the expert, MILO still successfully imitates with an extremely low number of expert state-action pairs while traditional offline IL methods such as behavior cloning (BC) fail completely. Source code is provided at https://github.com/jdchang1/milo.",
        "author": [
            "Jonathan D Chang",
            "Masatoshi Uehara",
            "Dhruv Sreenivas",
            "Rahul Kidambi",
            "Wen Sun"
        ],
        "id": 3,
        "recommendations": [
            {
                "id": 343,
                "paper_id": "441",
                "similarity": "0.9931141",
                "summary": "Online 3D Bin Packing with Constrained Deep Reinforcement Learning We solve a challenging yet practically useful variant of 3D Bin Packing Problem (3D-BPP). In our problem, the agent has limited information about the items to be packed into a single bin, and an item must be packed immediately after its arrival without buffering or readjusting. The item's place- ment also subjects to the constraints of order dependence and physical stability. We formulate this online 3D-BPP as a constrained Markov decision process (CMDP). To solve the problem, we propose an effective and easy-to-implement constrained deep reinforcement learning (DRL) method un- der the actor-critic framework. In particular, we introduce a prediction-and-projection scheme: The agent first predicts a feasibility mask for the placement actions as an auxiliary task and then uses the mask to modulate the action probabilities output by the actor during training. Such supervision and pro- jection facilitate the agent to learn feasible policies very effi- ciently. Our method can be easily extended to handle looka- head items, multi-bin packing, and item re-orienting. We have conducted extensive evaluation showing that the learned pol- icy significantly outperforms the state-of-the-art methods. A preliminary user study even suggests that our method might attain a human-level performance."
            },
            {
                "id": 11,
                "paper_id": "96",
                "similarity": "0.9921955",
                "summary": "Minimax Optimal Quantile and Semi-Adversarial Regret via Root-Logarithmic Regularizers Quantile (and, more generally, KL) regret bounds, such as those achieved by NormalHedge (Chaudhuri, Freund, and Hsu 2009) and its variants, relax the goal of competing against the best individual expert to only competing against a majority of experts on adversarial data. More recently, the semi-adversarial paradigm (Bilodeau, Negrea, and Roy 2020) provides an alternative relaxation of adversarial online learning by considering data that may be neither fully adversarial nor stochastic (i.i.d.). We achieve the minimax optimal regret in both paradigms using FTRL with separate, novel, root-logarithmic regularizers, both of which can be interpreted as yielding variants of NormalHedge. We extend existing KL regret upper bounds, which hold uniformly over target distributions, to possibly uncountable expert classes with arbitrary priors; provide the first full-information lower bounds for quantile regret on finite expert classes (which are tight); and provide an adaptively minimax optimal algorithm for the semi-adversarial paradigm that adapts to the true, unknown constraint faster, leading to uniformly improved regret bounds over existing methods."
            },
            {
                "id": 520,
                "paper_id": "629",
                "similarity": "0.99202394",
                "summary": "Drop-Bottleneck: LEARNING DISCRETE COMPRESSED REPRESENTATION FOR NOISE-ROBUST EXPLORATION We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. Drop- Bottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input vari- able, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DM- Lab (Beattie et al., 2016), our exploration method achieves state-of-the-art per- formance. As a new IB framework, we demonstrate that Drop-Bottleneck outper- forms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction."
            },
            {
                "id": 336,
                "paper_id": "434",
                "similarity": "0.99178517",
                "summary": "Apparently Irrational Choice as Optimal Sequential Decision Making In this paper, we propose a normative approach to model- ing apparently human irrational decision making (cognitive biases) that makes use of inherently rational computational mechanisms. We view preferential choice tasks as sequential decision making problems and formulate them as Partially Observable Markov Decision Processes (POMDPs). The re- sulting sequential decision model learns what information to gather about which options, whether to calculate option val- ues or make comparisons between options and when to make a choice. We apply the model to choice problems where con- text is known to influence human choice, an effect that has been taken as evidence that human cognition is irrational. Our results show that the new model approximates a bounded op- timal cognitive policy and makes quantitative predictions that correspond well to evidence about human choice. Further- more, the model uses context to help infer which option has a maximum expected value while taking into account com- putational cost and cognitive limits. In addition, it predicts when, and explains why, people stop evidence accumulation and make a decision. We argue that the model provides evi- dence that apparent human irrationalities are emergent conse- quences of processes that prefer higher value (rational) poli- cies."
            },
            {
                "id": 20,
                "paper_id": "105",
                "similarity": "0.9908949",
                "summary": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully- observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite."
            },
            {
                "id": 174,
                "paper_id": "268",
                "similarity": "0.990555",
                "summary": "Provably Efficient Exploration in Policy Optimization While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper pro- poses an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an \"optimistic version\" of the policy gradient di- rection. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves eO("
            },
            {
                "id": 97,
                "paper_id": "186",
                "similarity": "0.990545",
                "summary": "Q-value Path Decomposition for Deep Multiagent Reinforcement Learning Recently, deep multiagent reinforcement learning (MARL) has become a highly active research area as many real-world problems can be inherently viewed as multiagent systems. A particularly in- teresting and widely applicable class of problems is the partially observable cooperative multiagent setting, in which a team of agents learns to coor- dinate their behaviors conditioning on their pri- vate observations and commonly shared global reward signals. One natural solution is to resort to the centralized training and decentralized execu- tion paradigm and during centralized training, one key challenge is the multiagent credit assignment: how to allocate the global rewards for individ- ual agent policies for better coordination towards maximizing system-level's benefits. In this paper, we propose a new method called Q-value Path Decomposition (QPD) to decompose the system's global Q-values into individual agents' Q-values. Unlike previous works which restrict the represen- tation relation of the individual Q-values and the global one, we leverage the integrated gradient attribution technique into deep MARL to directly decompose global Q-values along trajectory paths to assign credits for agents. We evaluate QPD on the challenging StarCraft II micromanagement tasks and show that QPD achieves the state-of- the-art performance in both homogeneous and het- erogeneous multiagent scenarios compared with existing cooperative MARL algorithms."
            },
            {
                "id": 175,
                "paper_id": "269",
                "similarity": "0.99049395",
                "summary": "Goal-Aware Prediction: Learning to Model What Matters Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is ex- acerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning."
            },
            {
                "id": 546,
                "paper_id": "656",
                "similarity": "0.9898486",
                "summary": "PLANNING FROM PIXELS USING INVERSE DYNAMICS MODELS Learning task-agnostic dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn latent world models by learning to predict sequences of future actions conditioned on task completion. These task-conditioned models adaptively focus modeling capacity on task-relevant dynamics, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challeng- ing visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches."
            },
            {
                "id": 43,
                "paper_id": "128",
                "similarity": "0.98984456",
                "summary": "Exploiting Opponents under Utility Constraints in Sequential Games Recently, game-playing agents based on AI techniques have demonstrated super- human performance in several sequential games, such as chess, Go, and poker. Sur- prisingly, the multi-agent learning techniques that allowed to reach these achieve- ments do not take into account the actual behavior of the human player, potentially leading to an impressive gap in performances. In this paper, we address the problem of designing artificial agents that learn how to effectively exploit unknown human opponents while playing repeatedly against them in an online fashion. We study the case in which the agent's strategy during each repetition of the game is subject to constraints ensuring that the human's expected utility is within some lower and upper thresholds. Our framework encompasses several real-world problems, such as human engagement in repeated game playing and human education by means of serious games. As a first result, we formalize a set of linear inequalities encoding the conditions that the agent's strategy must satisfy at each iteration in order to do not violate the given bounds for the human's expected utility. Then, we use such formulation in an upper confidence bound algorithm, and we prove that the resulting procedure suffers from sublinear regret and guarantees that the constraints are satisfied with high probability at each iteration. Finally, we empirically evaluate the convergence of our algorithm on standard testbeds of sequential games."
            },
            {
                "id": 129,
                "paper_id": "219",
                "similarity": "0.98980105",
                "summary": "Learning To Stop While Learning To Predict There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stop- ping criteria for outputting results at different iter- ations, many algorithm-inspired deep models are restricted to a \"fixed-depth\" for all inputs. Similar to algorithms, the optimal depth of a deep architec- ture may be different for different input instances, either to avoid \"over-thinking\", or because we want to compute less for operations converged al- ready. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stop- ping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes per- spective and design a novel and effective training procedure which decomposes the task into an or- acle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, in- cluding learning sparse recovery, few-shot meta learning, and computer vision tasks."
            },
            {
                "id": 535,
                "paper_id": "645",
                "similarity": "0.9893681",
                "summary": "ACTING IN DELAYED ENVIRONMENTS WITH NON-STATIONARY MARKOV POLICIES The standard Markov Decision Process (MDP) formulation hinges on the assump- tion that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of m steps. The brute-force state augmentation baseline where the state is concatenated to the last m committed actions suffers from an exponential complexity in m, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks without resorting to state-augmentation. Experiments on tabular, physical, and Atari domains reveal that it converges quickly to high perfor- mance even for substantial delays, while standard approaches that either ignore the delay or rely on state-augmentation struggle or fail due to divergence. The code is available at https://github.com/galdl/rl_delay_basic.git."
            },
            {
                "id": 123,
                "paper_id": "213",
                "similarity": "0.9893404",
                "summary": "Distribution Augmentation for Generative Modeling We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmen- tation functions to data and, importantly, condi- tions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model ar- chitectures and problem domains."
            },
            {
                "id": 77,
                "paper_id": "164",
                "similarity": "0.9893232",
                "summary": "Minimax Regret for Stochastic Shortest Path We study the Stochastic Shortest Path (SSP) problem in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent has no prior knowledge about the costs and dynamics of the model. She repeatedly interacts with the model for K episodes, and has to minimize her regret. In this work we show that the minimax regret for this setting is �O( �"
            },
            {
                "id": 568,
                "paper_id": "678",
                "similarity": "0.9891798",
                "summary": "NEGATIVE DATA AUGMENTATION Data augmentation is often used to enlarge datasets with synthetic samples gen- erated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distri- bution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable con- ditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved con- ditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image clas- sification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks."
            },
            {
                "id": 324,
                "paper_id": "422",
                "similarity": "0.98878145",
                "summary": "Universal Trading for Order Execution with Oracle Policy Distillation As a fundamental problem in algorithmic trading, order exe- cution aims at fulfilling a specific trading order, either liqui- dation or acquirement, for a given instrument. Towards effec- tive execution strategy, recent years have witnessed the shift from the analytical view with model-based market assump- tions to model-free perspective, i.e., reinforcement learning, due to its nature of sequential decision optimization. How- ever, the noisy and yet imperfect market information that can be leveraged by the policy has made it quite challenging to build up sample efficient reinforcement learning methods to achieve effective order execution. In this paper, we propose a novel universal trading policy optimization framework to bridge the gap between the noisy yet imperfect market states and the optimal action sequences for order execution. Partic- ularly, this framework leverages a policy distillation method that can better guide the learning of the common policy to- wards practically optimal execution by an oracle teacher with perfect information to approximate the optimal trading strat- egy. The extensive experiments have shown significant im- provements of our method over various strong baselines, with reasonable trading actions."
            },
            {
                "id": 365,
                "paper_id": "463",
                "similarity": "0.9887704",
                "summary": "Differentially Private Link Prediction with Protected Connections Link prediction (LP) algorithms propose to each node a ranked list of nodes that are currently non-neighbors, as the most likely candidates for future linkage. Owing to increas- ing concerns about privacy, users (nodes) may prefer to keep some of their connections protected or private. Motivated by this observation, our goal is to design a differentially pri- vate LP algorithm, which trades off between privacy of the protected node-pairs and the link prediction accuracy. More specifically, we first propose a form of differential privacy on graphs, which models the privacy loss only of those node- pairs which are marked as protected. Next, we develop DPLP, a learning to rank algorithm, which applies a monotone trans- form to base scores from a non-private LP system, and then adds noise. DPLP is trained with a privacy induced ranking loss, which optimizes the ranking utility for a given maximum allowed level of privacy leakage of the protected node-pairs. Under a recently-introduced latent node embedding model, we present a formal trade-off between privacy and LP util- ity. Extensive experiments with several real-life graphs and several LP heuristics show that DPLP can trade off between privacy and predictive performance more effectively than sev- eral alternatives."
            },
            {
                "id": 74,
                "paper_id": "161",
                "similarity": "0.9887666",
                "summary": "Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning Federated learning (FL) has gain growing interests for its capability of learning from distributed data sources collectively without the need of accessing the raw data samples across different sources. So far FL research has mostly focused on improving the performance, how the algorithmic disparity will be impacted for the model learned from FL and the impact of algorithmic disparity on the utility inconsistency are largely unexplored. In this paper, we propose an FL framework to jointly consider performance consistency and algorithmic fairness across different local clients (data sources). We derive our framework from a constrained multi- objective optimization perspective, in which we learn a model satisfying fairness constraints on all clients with consistent performance. Specifically, we treat the algorithm prediction loss at each local client as an objective and maximize the worst-performing client with fairness constraints through optimizing a surrogate maximum function with all objectives involved. A gradient-based procedure is employed to achieve the Pareto optimality of this optimization problem. Theoretical analysis is provided to prove that our method can converge to a Pareto solution that achieves the min-max performance with fairness constraints on all clients. Comprehensive experiments on synthetic and real-world datasets demonstrate the superiority that our approach over baselines and its effectiveness in achieving both fairness and consistency across all local clients."
            },
            {
                "id": 124,
                "paper_id": "214",
                "similarity": "0.9887023",
                "summary": "Learning Discrete Structured Representations by Adversarially Maximizing Mutual Information We propose learning discrete structured represen- tations from unlabeled data by maximizing the mutual information between a structured latent variable and a target variable. Calculating mutual information is intractable in this setting. Our key technical contribution is an adversarial objective that can be used to tractably estimate mutual in- formation assuming only the feasibility of cross entropy calculation. We develop a concrete real- ization of this general formulation with Markov distributions over binary encodings. We report critical and unexpected findings on practical as- pects of the objective such as the choice of varia- tional priors. We apply our model on document hashing and show that it outperforms current best baselines based on discrete and vector quantized variational autoencoders. It also yields highly compressed interpretable representations."
            },
            {
                "id": 63,
                "paper_id": "150",
                "similarity": "0.9876188",
                "summary": "Replay-Guided Adversarial Environment Design Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self- supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR⊥, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR⊥ improves the performance of PAIRED, from which it inherited its theoretical framework."
            }
        ],
        "title": "Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage"
    }
]